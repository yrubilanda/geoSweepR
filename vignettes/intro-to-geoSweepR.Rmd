---
title: "intro-to-geoSweepR"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{intro-to-geoSweepR}
  %\VignetteEngine{rmarkdown::html_vignette}
  %\VignetteEncoding{UTF-8}
---

TAKES ABOUT 5 MINUTES TO RENDER SO FAR 4/30 11:00AM, 2 FUNCTIONS

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
library(geoSweepR)

eval_viz <- FALSE  # Turn this to TRUE later when you want to knit full plots

```

## Introduction

This vignette demonstrates how to use the `geoSweepR` package to visualize point-based geospatial data using smooth heatmaps. The package includes functions to generate 2D kernel density plots that highlight areas of high spatial concentration, with optional filtering by data type or category. Users can also overlay a basemap from external tile providers (e.g., CartoDB) to enhance spatial context.

At minimum, all functions in this package require that users input a `data` parameter, which should be a data frame containing point-based spatial data with clearly defined latitude and longitude columns. The names of these columns must be specified within the function using the parameters `lat_col` and `long_col`, which must be strings of data within the `data` parameter.

Optionally, users can also input filters to visualize a singular category or factor variable. This option is achieved by specifying the parameter `col_data`, which refers to the column used to filter the data set, and the parameter `filter_by`, which refers to a particular value within that column. Users should be aware that the filter option will not work, unless both `col_data` and `filter_by` are specified within the function. Users also have the option to overlay their heatmap or density plot on a spatial object, such as an `sf` or `raster` object by calling the parameter `basemap`.

Functions in this package also support coordinate reference system (CRS) customization through the `input_crs` parameter. Users can use this parameter by specifying an EPSG code or PROJ string. By default, all functions assume that `input_crs` is set to EPSG:4326 (WGS 84).

While this example uses crime incident data as a sample dataset, the tools provided in `geoSweepR` are suitable for a wide range of applications, including environmental sightings, service usage patterns, infrastructure reporting, and more. This example also uses a sample basemap, saved as a `.tif` file. Users can create or download their own basemaps to fit individual data sets.

The sample data set used here was obtained from the publicly available website DATA.GOV. You can find the data by clicking on this [link](https://catalog.data.gov/dataset/crime-incidents-in-2024).

```{r}
# Load sample data (replace with your actual data if needed)
data_path <- system.file("extdata", "dc_sample.csv", package = "geoSweepR") #YOU NEED TO USE THE DUMMY DATA IN THE EXTDATA FOLDER! THIS IS CRISTINAS CLEAN DATA
my_data <- readr::read_csv(data_path, col_names = TRUE)
head(my_data)
my_basemap <- terra::rast(system.file("extdata", "my_basemap.tif", package = "geoSweepR"))

```

## Function 1: make_heatmap by yrubilanda

The `make_heatmap()` function is the core tool in this package — it creates a smooth 2D heatmap using your point data (latitude and longitude). You can keep it simple and plot everything, or customize it with optional parameters to filter by type and add a basemap for context.

This function is flexible depending on what you want to show: - You can run it with just your coordinates and it’ll plot all your points. - You can filter the data by category using `col_data` and `filter_by`. - You can also include a basemap from map tiles (like CartoDB or OpenStreetMap) if you want something in the background.

Here are some ways to use it:

``` r
# Heatmap of all points (no filtering, no basemap)
make_heatmap(my_data, lat_col = "latitude", lon_col = "longitude")

# Heatmap filtered by a specific type (in this case, only "Theft" from the offense column)
make_heatmap(my_data, lat_col = "latitude", lon_col = "longitude",
             col_data = "offense", filter_by = "Theft")

# Heatmap with a basemap layer in the background
make_heatmap(my_data, lat_col = "latitude", lon_col = "longitude",
             basemap = my_basemap)
```

```{r, eval = eval_viz}
# This is the simplest version of the function — just make a heatmap of ALL your points
make_heatmap(
  data = my_data,           # your full dataset (must have lat/lon columns)
  lat_col = "latitude",  # column that holds the latitude values
  lon_col = "longitude", # column that holds the longitude values
  col_data = NULL,       # you also can just leave this blank so just do not write in
  filter_by = NULL,
  basemap = NULL
)

# NOTE:
# This does NOT filter by any category — it includes every row in your dataset.
# There's also no basemap in the background — just a clean heatmap showing areas with more data points.
# This is a great starting point if you just want to see the overall spatial density of your data.

```

```{r, eval = eval_viz}
# Call the heatmap function using just your point data — no basemap but you can filter by type within your column
make_heatmap(
  data = my_data,          # your dataset (must include lat/lon columns)
  lat_col = "latitude", # the column in your data that has latitude values
  lon_col = "longitude",# the column that has longitude values
  col_data = "offense", # this is the column we're filtering on (like type or category of data)
  filter_by = "Robbery" # only include rows where offense == "Robbery"
)
```

```{r, eval = eval_viz}
# Now use the heatmap function!
# This draws a density map of only "Robbery" incidents from the "offense" column,
# and overlays it on the sample basemap
make_heatmap(
  data = my_data,            # your dataset with coordinates and optional filter column
  lat_col = "latitude",      # name of the latitude column
  lon_col = "longitude",     # name of the longitude column
  col_data = "offense",      # column to filter by (can be type, category, etc.)
  filter_by = "Robbery",     # specific value to filter for (e.g., "Robbery")
  basemap = my_basemap       # the basemap image to use as a background (optional)
)

```

## Function 2: density_kriging by wompusjr

The `density_kriging()` function is a secondary tool for the analysis of spatial data — it creates a basic gridded density plot using your point data (longitude and latitude). This plot displays the predicted density of observations at unobserved tiles to extrapolate your results from the original heatmap to a larger area. The function comes with set grid size and variogram model type, but these are customizable - as is the CRS, which will be converted to metric from a provided CRS.

This function is designed to be used on categorical data by calculating the density of observations. By calculating density, you can use kriging methods that you would otherwise be incapable of doing by transforming the data into a continous variable with explanatory power.

The function also has some flexibility - You can run it with just your coordinates and it’ll produce a prediction from all of your points - You can filter the data by category using `col_data` and `filter_by`.

Here are some ways to use it:

``` r
# Plot density in standard grids and variogram types
density_kriging(my_data, lon_col = "longitude", lat_col = "latitude", crs = 4326)

# Plot density at a different grid size or variagram style - for example 250 m^2 grid and a gaussian variogram
density_kriging(my_data, lon_col = "longitude", lat_col = "latitude", crs = 4236, grid_size = 250, variogram_model = "Gau")

# Plot density with a just one kind of data -for example, only "Theft"
density_kriging(my_data, lon_col = "longitude", lat_col = "latitude", crs = 4326, col_data = "offense", filter_by = "Theft")
```

```{r, eval = eval_viz}
# Basic version of the function, running the entire code at standard levels
density_kriging(
  data = my_data,            # your full dataset (must have lat/lon columns)
  lon_col = "longitude", # column that holds the latitude values
  lat_col = "latitude",   # column that holds the longitude values
  input_crs = 4326,             #CRS for the data region - Washington DC 
  col_data = NULL,        #you can also leave this and the following sections blank to produce the base result
  filter_by = NULL, 
  grid_size = 500, 
  variogram_model = "Sph")

# NOTE:
# This does NOT filter by any category — it uses a count derived from your entire dataset.

# This is a good way to visualize how general trends in your data can be extrapolated beyond the recorded observation.

```

```{r, eval = eval_viz}
# Adjusted code displaying how using smaller grids could affect the data
density_kriging(
  data = my_data,            # your full dataset (must have lat/lon columns)
  lon_col = "longitude", # column that holds the latitude values
  lat_col = "latitude",   # column that holds the longitude values
  input_crs = 4326,             #CRS for the data region - Washington DC 
  grid_size = 250)

# NOTE:
# This still does NOT filter by any category — it uses a count derived from your entire dataset but uses smaller grid sizes to get more granular with the visualization

# This can be useful for smaller datasets and for visualizing boundaries

```

```{r, eval = eval_viz}
# You can filter by specific values in particular categorical variables
density_kriging(
  data = my_data,            # your full dataset (must have lat/lon columns)
  lon_col = "longitude", # column that holds the latitude values
  lat_col = "latitude",   # column that holds the longitude values
  input_crs = 4326,             #CRS for the data region - Washington DC 
  col_data = "offense",     
  filter_by = "Robbery")

# NOTE:
# This is a good way to visualize a specific categorical value or variable
```

## Function 3: resample_heatmap() by cgarcialeal5

The `resample_heatmap()` function in the `geoSweepR` package is designed as an alternative to the core function `make_heatmap()`. It is designed to handle situations where the geospatial data set is insufficient or limited. In many real-life scenarios, data sets may not have enough spatial coverage or resolution to generate meaningful visualizations. This function addresses that issue by resampling the data set to increase its spatial density and enable more informative heatmap visualizations. It artificially creates new data points by resampling within the spatial boundaries of the original data set. The output of this function includes two objects: a `ggplot` object with a heatmap and a `data frame` object with the resampled data. The new data frame contains the same columns as the original data and two new columns called `x` and `y`, which are created during the execution of the function.

The parameter `n_samples` is unique to this function and gives users control over the level of resampling by specifying the number of samples to generate. If this parameter is left unspecified, the function defaults to creating a significantly denser data set by multiplying the number of rows in the data set by 1000. If the user inputs the `filter_by` parameter, the resampling will use the filtered data as if it represents the whole data set.

### Here are some ways to use it:

#### Resampling the whole data set with no other parameters

This is the most basic way to use this function. It will default multiplying the number of rows in the entire data set (\~25,000) by 1000. It should only be used when the data set it small and limited in relation to the geographic area that is covered.

```{r, eval = eval_viz}
resample_heatmap(
  data = my_data, 
  lat_col = "latitude", 
  lon_col = "longitude"
  )
```

#### Resampling the filtered data set without `n_samples` parameter

The function will now multiply the number of rows that have value specified in the `filter_by` parameter in the column identified by the `col_data` parameter by 1000. In this example, the function is filtering the data set to only the rows that contain `Homicide` as the value in the column `offense`. Thus, the data set is reduced to \~150 rows from the original \~25,000. Since the `n_sample` parameter was not used, the function will multiply the \~150 rows by 1000. The rest of the function will run as if `n_samples` was defined as \~150,000.

```{r, eval = eval_viz}
resample_heatmap(
  data = my_data, 
  lat_col = "latitude", 
  lon_col = "longitude",
  col_data = "offense", 
  filter_by = "Homicide"
  )
```

#### Resampling the filtered data set with `n_samples` parameter and basemap

With the addition of the `n_sample` parameter, the function no longer multiplies the number of rows with the `filter_by` value by 1000. In this example, the function is filtering the data set to only the rows that contain `Homicide` as the value in the column `offense`. Thus, the data set is reduced to \~150 rows from the original \~25,000. Since the `n_sample` parameter was used, the function resamples the \~150 original rows a total of 20,000 times. This parameter is useful when the optional filter results in a very small subset of data. Lastly, in this example the parameter for `basemap` is also used, so the resulting heatmap will be overlaid on `my_basemap`.

```{r, eval = eval_viz}
resample_heatmap(
  data = my_data, 
  lat_col = "latitude", 
  lon_col = "longitude", 
  col_data = "offense", 
  filter_by = "Homicide", 
  n_samples = 20000,
  basemap = my_basemap
  )
```
